{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2493c5f",
   "metadata": {},
   "source": [
    "# Section 0: Overview\n",
    "\n",
    "Active learning - way to train models faster with fewer labels. We do this by allowing the model to choose which unlabeled examples to \"ask about next\". \n",
    "\n",
    "- Instead of just labeling everything, we prioritise the most useful items first - typically the ones that a model is most unsure about, or potentially items that represent some new pattern (distribution).\n",
    "- 20 questions analogy - narrow down as efficiently as possible with \"smart\" questions. \n",
    "\n",
    "Labeling data = expensive \n",
    "\n",
    "If we can label fewer items, we save money/time - we're focusing attention on the most informative examples. \n",
    "\n",
    "Sustainable - less compute bc learning more efficiently = good for the planet! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b919f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. generate a moon dataset\n",
    "X, y = make_moons(n_samples=600, noise=0.25, random_state=42)\n",
    "\n",
    "# 2. start with just a small labeled set (simulate \"what we already know\")\n",
    "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.9, stratify=y, random_state=42)\n",
    "\n",
    "print(f\"Initial training set size: {X_train.shape[0]} samples\")\n",
    "\n",
    "# 3. fit a really simple logistic regression model \n",
    "clf = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "\n",
    "# 4. build a grid (visualise our predictions)\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 100),\n",
    "    np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 100)\n",
    ")\n",
    "\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "# 5. define an \"uncertain\" band - where the model is unsure (i.e. near 50/50)\n",
    "uncertain = (probs > 0.45) & (probs < 0.55)\n",
    "\n",
    "# 6. plot\n",
    "\n",
    "# set-up and shading! \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, probs, levels=20, alpha=0.35)\n",
    "plt.contourf(xx, yy, uncertain, levels=[0.5, 1], alpha=0.25)\n",
    "\n",
    "# plot our points\n",
    "plt.scatter(X_pool[:, 0], X_pool[:, 1], c='gray', s=12, label='Unlabeled Pool', alpha=0.5)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=\"coolwarm\", s=45, label='Labeled', alpha=0.8, edgecolor='k')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Which points should we label next?\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "active-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
